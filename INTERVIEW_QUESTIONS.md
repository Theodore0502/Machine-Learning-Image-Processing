# 30 C√¢u H·ªèi Gi·∫£ng Vi√™n C√≥ Th·ªÉ H·ªèi - Rice Leaf Disease Detection

## üìä Ph√¢n lo·∫°i theo ƒë·ªô kh√≥

### üü¢ C∆† B·∫¢N (C√¢u 1-10) - Ki·∫øn th·ª©c n·ªÅn t·∫£ng

#### 1. **M·ª•c ti√™u c·ªßa d·ª± √°n n√†y l√† g√¨?**
**Tr·∫£ l·ªùi**: Ph√°t tri·ªÉn h·ªá th·ªëng AI ph√°t hi·ªán v√† ph√¢n lo·∫°i b·ªánh tr√™n l√° l√∫a s·ª≠ d·ª•ng deep learning, h·ªó tr·ª£ n√¥ng d√¢n ch·∫©n ƒëo√°n b·ªánh s·ªõm ƒë·ªÉ c√≥ bi·ªán ph√°p ƒëi·ªÅu tr·ªã k·ªãp th·ªùi.

#### 2. **D·ª± √°n n√†y gi·∫£i quy·∫øt b√†i to√°n g√¨ trong Machine Learning?**
**Tr·∫£ l·ªùi**: B√†i to√°n ph√¢n lo·∫°i ·∫£nh (Image Classification) - m·ªôt d·∫°ng supervised learning, c·ª• th·ªÉ l√† multi-class classification v·ªõi 5 l·ªõp b·ªánh kh√°c nhau.

#### 3. **Dataset c·ªßa b·∫°n c√≥ bao nhi√™u l·ªõp? K·ªÉ t√™n c√°c l·ªõp.**
**Tr·∫£ l·ªùi**: 5 l·ªõp:
- bacterial_blight (B·∫°c l√° do vi khu·∫©n)
- blast (ƒê·∫°o √¥n)
- brown_spot (ƒê·ªëm n√¢u)
- healthy (Kh·ªèe m·∫°nh)
- tungro (B·ªánh v√†ng l√πn)

#### 4. **Dataset ƒë∆∞·ª£c chia th√†nh bao nhi√™u t·∫≠p? T·ª∑ l·ªá chia nh∆∞ th·∫ø n√†o?**
**Tr·∫£ l·ªùi**: 3 t·∫≠p:
- **Train**: ~70% - hu·∫•n luy·ªán model
- **Validation**: ~15% - tune hyperparameters v√† early stopping
- **Test**: ~15% - ƒë√°nh gi√° cu·ªëi c√πng

#### 5. **T·∫°i sao ph·∫£i normalize ·∫£nh v·ªõi mean=(0.485, 0.456, 0.406) v√† std=(0.229, 0.224, 0.225)?**
**Tr·∫£ l·ªùi**: ƒê√¢y l√† gi√° tr·ªã mean/std c·ªßa **ImageNet dataset**. V√¨ model s·ª≠ d·ª•ng pretrained weights t·ª´ ImageNet, vi·ªác normalize theo c√πng distribution gi√∫p model ho·∫°t ƒë·ªông t·ªët h∆°n (transfer learning best practice).

#### 6. **Input size c·ªßa model l√† bao nhi√™u? T·∫°i sao ch·ªçn size ƒë√≥?**
**Tr·∫£ l·ªùi**: 224x224 pixels. ƒê√¢y l√† standard size cho:
- Vision Transformer variants (ViT-Small patch16_224)
- C√¢n b·∫±ng gi·ªØa chi ti·∫øt h√¨nh ·∫£nh v√† hi·ªáu nƒÉng t√≠nh to√°n
- Compatible v·ªõi pretrained weights

#### 7. **Loss function b·∫°n s·ª≠ d·ª•ng l√† g√¨? T·∫°i sao?**
**Tr·∫£ l·ªùi**: **CrossEntropyLoss**
- Ph√π h·ª£p cho multi-class classification
- K·∫øt h·ª£p softmax + negative log likelihood
- T·ªëi ∆∞u h√≥a ph√¢n ph·ªëi x√°c su·∫•t gi·ªØa c√°c l·ªõp

#### 8. **Optimizer b·∫°n d√πng l√† g√¨? Learning rate bao nhi√™u?**
**Tr·∫£ l·ªùi**: 
- **AdamW optimizer** (Adam with weight decay)
- Learning rate ban ƒë·∫ßu: **1e-4** cho CNN, **5e-5** cho ViT
- AdamW t·ªët h∆°n Adam cho Vision Transformers v√¨ t√°ch ri√™ng weight decay

#### 9. **Metrics b·∫°n d√πng ƒë·ªÉ ƒë√°nh gi√° model l√† g√¨?**
**Tr·∫£ l·ªùi**: 
- **Accuracy**: T·ª∑ l·ªá d·ª± ƒëo√°n ƒë√∫ng t·ªïng th·ªÉ
- **F1-Score (macro)**: Trung b√¨nh ƒëi·ªÅu h√≤a c·ªßa Precision v√† Recall, quan tr·ªçng khi dataset c√≥ th·ªÉ imbalanced
- **Confusion Matrix**: Ph√¢n t√≠ch chi ti·∫øt l·ªói d·ª± ƒëo√°n

#### 10. **Gradio l√† g√¨? T·∫°i sao d√πng Gradio?**
**Tr·∫£ l·ªùi**: 
- Framework Python ƒë·ªÉ t·∫°o web UI cho ML models
- ∆Øu ƒëi·ªÉm: D·ªÖ d√πng, nhanh ch√≥ng, kh√¥ng c·∫ßn vi·∫øt HTML/CSS/JS, t√≠ch h·ª£p t·ªët v·ªõi PyTorch
- Ph√π h·ª£p cho demo v√† prototype

---

### üü° TRUNG B√åNH (C√¢u 11-20) - Ki·∫øn th·ª©c chuy√™n s√¢u

#### 11. **So s√°nh CNN v√† Vision Transformer trong d·ª± √°n c·ªßa b·∫°n.**
**Tr·∫£ l·ªùi**:
| Ti√™u ch√≠ | CNN (SmallCNN) | ViT (Small) |
|----------|----------------|-------------|
| **F1-Score** | 85.7% | 87.6% |
| **Accuracy** | 87.3% | 89.2% |
| **T·ªëc ƒë·ªô** | ~15-20ms | ~50-100ms |
| **K√≠ch th∆∞·ªõc** | ~1.5MB | ~87MB |
| **C∆° ch·∫ø** | Convolution + pooling | Self-attention |
| **Inductive bias** | Locality & translation invariance | Minimal inductive bias |
| **Data efficiency** | T·ªët v·ªõi √≠t data | C·∫ßn nhi·ªÅu data ho·∫∑c pretrained |

#### 12. **Self-attention trong ViT ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o?**
**Tr·∫£ l·ªùi**:
1. Chia ·∫£nh th√†nh patches (16x16 pixels)
2. Flatten m·ªói patch th√†nh vector
3. Add positional embedding
4. M·ªói patch "attend" ƒë·∫øn t·∫•t c·∫£ patches kh√°c qua Q, K, V matrices
5. T√≠nh attention weights: softmax(QK^T / ‚àöd)
6. Weighted sum c·ªßa V theo attention weights
7. Cho ph√©p model h·ªçc global dependencies

#### 13. **Data augmentation b·∫°n s·ª≠ d·ª•ng l√† g√¨? T·∫°i sao?**
**Tr·∫£ l·ªùi** (trong `datasets_cls.py`):
- **RandomResizedCrop**: M√¥ ph·ªèng c√°c g√≥c ch·ª•p kh√°c nhau
- **RandomHorizontalFlip**: L√° c√≥ th·ªÉ l·∫≠t ngang t·ª± nhi√™n
- **ColorJitter** (brightness, contrast, saturation): ƒêi·ªÅu ki·ªán √°nh s√°ng kh√°c nhau
- **RandomRotation**: G√≥c quay c·ªßa l√° khi ch·ª•p
- M·ª•c ƒë√≠ch: TƒÉng diversity, gi·∫£m overfitting, model robust h∆°n

#### 14. **Early stopping ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o trong d·ª± √°n?**
**Tr·∫£ l·ªùi** (trong `engine.py`):
```python
patience = 10  # Ch·ªù 10 epochs
```
- Track validation F1-score m·ªói epoch
- N·∫øu F1 kh√¥ng tƒÉng sau 10 epochs li√™n ti·∫øp ‚Üí d·ª´ng training
- L∆∞u checkpoint c·ªßa epoch c√≥ F1 cao nh·∫•t
- Tr√°nh overfitting v√† ti·∫øt ki·ªám th·ªùi gian

#### 15. **Learning rate scheduler b·∫°n d√πng l√† g√¨? Gi·∫£i th√≠ch.**
**Tr·∫£ l·ªùi**: **CosineAnnealingLR**
```python
scheduler = CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)
```
- Learning rate gi·∫£m theo d·∫°ng cosine t·ª´ lr ban ƒë·∫ßu v·ªÅ eta_min
- Smooth convergence, tr√°nh oscillation
- Ph√π h·ª£p cho vision tasks

#### 16. **Confidence threshold 60% c√≥ √Ω nghƒ©a g√¨?**
**Tr·∫£ l·ªùi** (trong `validation.py`):
- **< 60%**: üî¥ Low confidence - C√≥ th·ªÉ KH√îNG ph·∫£i l√° l√∫a ho·∫∑c ·∫£nh k√©m ch·∫•t l∆∞·ª£ng
- **60-80%**: üü° Medium confidence - Ch·∫•p nh·∫≠n ƒë∆∞·ª£c
- **‚â• 80%**: üü¢ High confidence - R·∫•t tin c·∫≠y

L√Ω do: D·ª±a tr√™n ph√¢n t√≠ch validation set, model v·ªõi confidence < 60% th∆∞·ªùng d·ª± ƒëo√°n sai ho·∫∑c ·∫£nh kh√¥ng ph·∫£i rice leaf.

#### 17. **T·∫°i sao ViT c·∫ßn pretrained weights nhi·ªÅu h∆°n CNN?**
**Tr·∫£ l·ªùi**:
- **CNN**: C√≥ inductive bias m·∫°nh (locality, translation invariance) ‚Üí h·ªçc t·ªët t·ª´ √≠t data
- **ViT**: Minimal inductive bias, coi ·∫£nh nh∆∞ sequence ‚Üí c·∫ßn nhi·ªÅu data ƒë·ªÉ h·ªçc pattern
- Pretrained tr√™n ImageNet (1.2M ·∫£nh) gi√∫p ViT c√≥ starting point t·ªët
- V·ªõi small dataset (~1000 ·∫£nh), pretrained ViT v∆∞·ª£t tr·ªôi CNN train from scratch

#### 18. **Gi·∫£i th√≠ch Auto Color Normalization trong d·ª± √°n.**
**Tr·∫£ l·ªùi** (trong `color_normalization.py`):
```python
def auto_normalize_leaf(image):
    # Convert RGB ‚Üí HSV
    # Adjust hue v·ªÅ xanh l√° chu·∫©n (target_hue ~ 100-110¬∞)
    # Adjust saturation v·ªÅ m·ª©c v·ª´a ph·∫£i
    # Adjust brightness v·ªÅ optimal range
```
- **M·ª•c ƒë√≠ch**: Chu·∫©n h√≥a m√†u s·∫Øc l√° v·ªÅ ƒëi·ªÅu ki·ªán chu·∫©n
- **L·ª£i √≠ch**: Gi·∫£m ·∫£nh h∆∞·ªüng c·ªßa √°nh s√°ng, camera kh√°c nhau ‚Üí tƒÉng accuracy
- **Trade-off**: C√≥ th·ªÉ m·∫•t th√¥ng tin m√†u s·∫Øc quan tr·ªçng c·ªßa b·ªánh

#### 19. **Class imbalance ƒë∆∞·ª£c x·ª≠ l√Ω nh∆∞ th·∫ø n√†o?**
**Tr·∫£ l·ªùi**:
- **Ph√¢n t√≠ch**: D√πng `dataset_stats.py` ƒë·ªÉ visualize distribution
- **Stratified sampling**: Train/val/test split gi·ªØ t·ª∑ l·ªá m·ªói l·ªõp
- **Data augmentation**: TƒÉng c∆∞·ªùng cho l·ªõp thi·ªÉu s·ªë
- **Macro F1-score**: ƒê√°nh gi√° c√¥ng b·∫±ng c√°c l·ªõp (kh√¥ng b·ªã bias v·ªÅ l·ªõp ƒëa s·ªë)
- **Weighted sampling** (n·∫øu c·∫ßn): Sample nhi·ªÅu h∆°n t·ª´ l·ªõp thi·ªÉu s·ªë

#### 20. **Checkpoint l∆∞u g√¨? C·∫•u tr√∫c nh∆∞ th·∫ø n√†o?**
**Tr·∫£ l·ªùi** (trong `engine.py`):
```python
checkpoint = {
    "model": model.state_dict(),
    "optimizer": optimizer.state_dict(),
    "scheduler": scheduler.state_dict(),
    "epoch": epoch,
    "best_f1": best_f1,
    "config": {...}
}
```
- L∆∞u best checkpoint (highest F1) v√† last checkpoint
- Cho ph√©p resume training ho·∫∑c inference

---

### üî¥ KH√ì (C√¢u 21-30) - C√¢u h·ªèi n√¢ng cao & t√¨nh hu·ªëng

#### 21. **N·∫øu model overfitting, b·∫°n s·∫Ω l√†m g√¨?**
**Tr·∫£ l·ªùi**:
1. **TƒÉng regularization**:
   - TƒÉng weight decay (L2 regularization)
   - Th√™m Dropout layers
2. **Data augmentation m·∫°nh h∆°n**: Th√™m MixUp, CutMix
3. **Early stopping**: Gi·∫£m patience
4. **Gi·∫£m model complexity**: D√πng model nh·ªè h∆°n
5. **Th√™m data**: Collect th√™m ·∫£nh th·ª±c t·∫ø
6. **Label smoothing**: Gi·∫£m overconfidence

#### 22. **Gi·∫£i th√≠ch backpropagation trong Transformer.**
**Tr·∫£ l·ªùi**:
1. Forward pass: Input ‚Üí Multi-head attention ‚Üí MLP ‚Üí Output
2. Compute loss v·ªõi ground truth
3. Backward pass:
   - Gradient flow qua softmax classification head
   - Qua LayerNorm v√† residual connections
   - Qua Multi-head attention (gradient c·ªßa Q, K, V matrices)
   - Qua patch embedding v√† positional encoding
4. **Residual connections** gi√∫p gradient flow t·ªët h∆°n (tr√°nh vanishing gradient)
5. **LayerNorm** stabilize training

#### 23. **T·∫°i sao d√πng F1-score thay v√¨ Accuracy l√†m metric ch√≠nh?**
**Tr·∫£ l·ªùi**:
- **Accuracy**: C√≥ th·ªÉ misleading khi imbalanced
  - VD: 90% healthy, 10% diseased ‚Üí model d·ª± ƒëo√°n "healthy" cho t·∫•t c·∫£ v·∫´n c√≥ 90% accuracy
- **F1-score (macro)**:
  - Trung b√¨nh ƒëi·ªÅu h√≤a c·ªßa Precision v√† Recall
  - ƒê√°nh gi√° c√¥ng b·∫±ng m·ªçi l·ªõp, k·ªÉ c·∫£ thi·ªÉu s·ªë
  - F1 = 2 √ó (Precision √ó Recall) / (Precision + Recall)
- **Trong y t·∫ø/n√¥ng nghi·ªáp**: Kh√¥ng b·ªè s√≥t b·ªánh (high recall) quan tr·ªçng

#### 24. **Gi·∫£i th√≠ch gradient descent v√† momentum trong optimizer.**
**Tr·∫£ l·ªùi**:
- **Gradient Descent**: Œ∏ = Œ∏ - lr √ó ‚àáL
- **Momentum** (trong Adam):
  ```
  m_t = Œ≤1 √ó m_{t-1} + (1-Œ≤1) √ó ‚àáL      # First moment (mean)
  v_t = Œ≤2 √ó v_{t-1} + (1-Œ≤2) √ó (‚àáL)¬≤   # Second moment (variance)
  Œ∏ = Œ∏ - lr √ó m_t / (‚àöv_t + Œµ)
  ```
- **Œ≤1=0.9, Œ≤2=0.999**: Smooth gradient, adaptive learning rate
- **AdamW**: Decouple weight decay kh·ªèi gradient step

#### 25. **Transfer learning ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o? T·∫°i sao hi·ªáu qu·∫£?**
**Tr·∫£ l·ªùi**:
- **C∆° ch·∫ø**:
  1. Load pretrained weights t·ª´ ImageNet
  2. Freeze early layers (h·ªçc features c∆° b·∫£n: edges, textures)
  3. Fine-tune later layers + classification head
- **Hi·ªáu qu·∫£ v√¨**:
  - Low-level features (edges, colors) t∆∞∆°ng t·ª± gi·ªØa datasets
  - Kh√¥ng c·∫ßn h·ªçc l·∫°i t·ª´ ƒë·∫ßu
  - √çt data v·∫´n converge t·ªët
- **Trong d·ª± √°n**: CNN v√† ViT ƒë·ªÅu d√πng pretrained ‚Üí F1 ~87% v·ªõi ch·ªâ ~1000 ·∫£nh

#### 26. **Batch size ·∫£nh h∆∞·ªüng nh∆∞ th·∫ø n√†o ƒë·∫øn training?**
**Tr·∫£ l·ªùi**:
- **Batch size nh·ªè (8-16)**:
  - ‚úÖ Gradient noisy h∆°n ‚Üí regularization effect, tr√°nh sharp minima
  - ‚úÖ D√πng √≠t VRAM
  - ‚ùå Slow, nhi·ªÅu iterations
- **Batch size l·ªõn (64-128)**:
  - ‚úÖ Fast, stable gradient
  - ‚úÖ T·∫≠n d·ª•ng GPU parallelism
  - ‚ùå C·∫ßn nhi·ªÅu VRAM
  - ‚ùå C√≥ th·ªÉ converge ƒë·∫øn sharp minima (generalize k√©m)
- **Trong d·ª± √°n**: Batch size 16 (c√¢n b·∫±ng speed v√† stability cho dataset nh·ªè)

#### 27. **N·∫øu model b·ªã bias v·ªÅ m·ªôt l·ªõp, l√†m sao fix?**
**Tr·∫£ l·ªùi**:
1. **Root cause**: Imbalanced data ho·∫∑c l·ªõp ƒë√≥ d·ªÖ ph√¢n bi·ªát
2. **Solutions**:
   - **Weighted loss**: G√°n tr·ªçng s·ªë cao cho l·ªõp thi·ªÉu s·ªë
     ```python
     weights = 1 / class_counts
     criterion = nn.CrossEntropyLoss(weight=weights)
     ```
   - **Focal loss**: Focus v√†o hard examples
   - **Oversampling**: Duplicate minority class
   - **Class-balanced augmentation**: Aug m·∫°nh h∆°n cho minority
   - **Two-stage training**: Train l·∫°i classification head v·ªõi balanced batch

#### 28. **Gi·∫£i th√≠ch Confusion Matrix v√† c√°ch s·ª≠ d·ª•ng.**
**Tr·∫£ l·ªùi**:
```
              Predicted
           Blast  Healthy  ...
Actual  
Blast      80      5       ...   ‚Üê True Positives, False Negatives
Healthy     3     90       ...
```

**Ph√¢n t√≠ch**:
- **Diagonal**: D·ª± ƒëo√°n ƒë√∫ng
- **Off-diagonal**: Confusion gi·ªØa c√°c l·ªõp
- **Pattern quan tr·ªçng**:
  - N·∫øu "healthy" th∆∞·ªùng b·ªã nh·∫ßm th√†nh "brown_spot" ‚Üí 2 l·ªõp n√†y t∆∞∆°ng ƒë·ªìng cao
  - N·∫øu "blast" √≠t b·ªã nh·∫ßm ‚Üí model ph√¢n bi·ªát t·ªët b·ªánh nghi√™m tr·ªçng
  
**Actions**:
- Thu th·∫≠p th√™m data cho c·∫∑p b·ªã confused
- Augmentation target v√†o c·∫∑p ƒë√≥
- Feature engineering: t√¨m features ph√¢n bi·ªát 2 l·ªõp

#### 29. **Deployment: L√†m sao optimize model cho production?**
**Tr·∫£ l·ªùi**:
1. **Model Quantization**:
   - FP32 ‚Üí FP16 or INT8
   - Gi·∫£m 2-4x size, tƒÉng 2-3x speed
   - Trade-off: Accuracy gi·∫£m ~0.5-1%
   
2. **Model Pruning**:
   - Lo·∫°i b·ªè weights/neurons kh√¥ng quan tr·ªçng
   - Structured pruning: Lo·∫°i channels/layers
   
3. **Knowledge Distillation**:
   - Train model nh·ªè (student) h·ªçc t·ª´ model l·ªõn (teacher)
   - VD: ViT (teacher) ‚Üí SmallCNN (student)
   
4. **ONNX Export**:
   ```python
   torch.onnx.export(model, dummy_input, "model.onnx")
   ```
   - Cross-platform, t√≠ch h·ª£p v·ªõi TensorRT, OpenVINO
   
5. **Caching & Batching**:
   - Batch nhi·ªÅu requests c√πng l√∫c
   - Cache predictions cho ·∫£nh ƒë√£ th·∫•y

#### 30. **Thi·∫øt k·∫ø pipeline end-to-end t·ª´ ·∫£nh ƒë·∫øn d·ª± ƒëo√°n.**
**Tr·∫£ l·ªùi**:
```
1. IMAGE ACQUISITION
   ‚îú‚îÄ User upload qua Gradio UI
   ‚îú‚îÄ Validation: file format, size
   ‚îî‚îÄ Load to PIL Image

2. PREPROCESSING
   ‚îú‚îÄ [Optional] Manual adjustments: brightness, contrast, rotation
   ‚îú‚îÄ [Optional] Auto color normalization (HSV correction)
   ‚îú‚îÄ Resize to 224x224
   ‚îú‚îÄ ToTensor: [0,255] ‚Üí [0,1]
   ‚îî‚îÄ Normalize: ImageNet mean/std

3. INFERENCE
   ‚îú‚îÄ Load pretrained model (CNN or ViT)
   ‚îú‚îÄ model.eval() + torch.no_grad()
   ‚îú‚îÄ Forward pass: image ‚Üí logits
   ‚îî‚îÄ Softmax: logits ‚Üí probabilities

4. POST-PROCESSING
   ‚îú‚îÄ ArgMax: get predicted class index
   ‚îú‚îÄ Confidence check: threshold validation
   ‚îú‚îÄ Top-K predictions: argsort probabilities
   ‚îî‚îÄ Disease status: healthy vs diseased

5. VISUALIZATION
   ‚îú‚îÄ Display predicted class + confidence
   ‚îú‚îÄ Bar chart: probability distribution
   ‚îú‚îÄ Warning messages: low confidence
   ‚îî‚îÄ [Optional] Model comparison (CNN vs ViT)

6. OUTPUT
   ‚îú‚îÄ Markdown formatted results
   ‚îú‚îÄ Probability plots
   ‚îî‚îÄ [Optional] Save to JSON/CSV
```

**Critical considerations**:
- **Latency**: ~50-100ms cho ViT, ~15-20ms cho CNN
- **Error handling**: Invalid images, low confidence
- **Logging**: Track predictions for monitoring
- **Feedback loop**: Collect user feedback for retraining

---

## üéØ M·∫πo chu·∫©n b·ªã ph·ªèng v·∫•n

### Hi·ªÉu s√¢u 3 kh√≠a c·∫°nh:
1. **L√Ω thuy·∫øt ML**: Loss, optimizer, metrics, architectures
2. **Implementation**: Code structure, libraries, best practices  
3. **Domain knowledge**: Rice diseases, practical deployment

### Lu√¥n c√≥ v√≠ d·ª• c·ª• th·ªÉ:
- ƒê·ª´ng ch·ªâ n√≥i "d√πng data augmentation"
- N√≥i "d√πng RandomHorizontalFlip v·ªõi p=0.5 v√¨ l√° l√∫a c√≥ th·ªÉ xu·∫•t hi·ªán theo nhi·ªÅu h∆∞·ªõng"

### Chu·∫©n b·ªã demo:
- Ch·∫°y Gradio app th√†nh th·∫°o
- Test v·ªõi c·∫£ ·∫£nh t·ªët v√† ·∫£nh x·∫•u
- So s√°nh CNN vs ViT tr·ª±c ti·∫øp

### Bi·∫øt ƒëi·ªÉm m·∫°nh/y·∫øu:
- **M·∫°nh**: Dual model, visualization t·ªët, pipeline ho√†n ch·ªânh
- **Y·∫øu**: Dataset nh·ªè, ch∆∞a deploy production, ch∆∞a mobile app

**Ch√∫c b·∫°n b·∫£o v·ªá th√†nh c√¥ng! üöÄ**

---

## üßÆ L√ù THUY·∫æT V·ªÄ THU·∫¨T TO√ÅN & √ÅP D·ª§NG TRONG D·ª∞ √ÅN

### üìò PH·∫¶N 1: CNN (Convolutional Neural Network)

#### 1.1 L√Ω Thuy·∫øt To√°n H·ªçc

**Convolution Operation**:
```
Output[i,j] = Œ£ Œ£ Input[i+m, j+n] √ó Kernel[m,n] + bias
             m n
```

**V√≠ d·ª• c·ª• th·ªÉ**:
- Input: 224√ó224√ó3 (RGB image)
- Kernel: 3√ó3√ó3√ó64 (64 filters, m·ªói filter 3√ó3 tr√™n 3 channels)
- Output: 224√ó224√ó64 (v·ªõi padding='same')

**C√°c th√†nh ph·∫ßn ch√≠nh**:

1. **Convolution Layer**:
   - Chi·∫øt xu·∫•t features c·ª•c b·ªô (edges, textures, patterns)
   - Shared weights ‚Üí translation invariance
   - Formula: y = œÉ(W * x + b) 
     - W: filter weights
     - *: convolution operation
     - œÉ: activation (ReLU)

2. **Pooling Layer**:
   - Max pooling: f(x) = max(x_i) trong window
   - Down-sampling: gi·∫£m spatial dimensions
   - TƒÉng receptive field, gi·∫£m computation

3. **Activation Function (ReLU)**:
   - ReLU(x) = max(0, x)
   - Gi·∫£i quy·∫øt vanishing gradient
   - Sparse activation (hi·ªáu qu·∫£ t√≠nh to√°n)

4. **Batch Normalization**:
   ```
   BN(x) = Œ≥ √ó (x - Œº_batch) / ‚àö(œÉ¬≤_batch + Œµ) + Œ≤
   ```
   - Normalize activations
   - Stabilize training, tƒÉng learning rate ƒë∆∞·ª£c

#### 1.2 √Åp D·ª•ng Trong D·ª± √Ån - SmallCNN

**Architecture** (trong `src/models/cnn_small.py`):
```python
class SmallCNN(nn.Module):
    def __init__(self, num_classes=5):
        # Block 1: 3√ó3 conv ‚Üí BN ‚Üí ReLU ‚Üí MaxPool
        Conv2d(3, 32, kernel_size=3, padding=1)
        BatchNorm2d(32)
        ReLU()
        MaxPool2d(2, 2)  # 224‚Üí112
        
        # Block 2: 32‚Üí64 channels
        Conv2d(32, 64, 3, 1)
        # ... ‚Üí 112‚Üí56
        
        # Block 3: 64‚Üí128 channels
        # ... ‚Üí 56‚Üí28
        
        # Block 4: 128‚Üí256 channels
        # ... ‚Üí 28‚Üí14
        
        # Global Average Pooling: 14√ó14√ó256 ‚Üí 1√ó1√ó256
        AdaptiveAvgPool2d(1)
        
        # FC layer: 256 ‚Üí 5 classes
        Linear(256, num_classes)
```

**T·∫°i sao thi·∫øt k·∫ø nh∆∞ v·∫≠y?**:
- **4 blocks v·ªõi increasing channels (32‚Üí64‚Üí128‚Üí256)**: 
  - Early layers: low-level features (edges, colors)
  - Later layers: high-level features (disease patterns)
  
- **Kernel size 3√ó3**: Standard, c√¢n b·∫±ng receptive field v√† parameters

- **Global Average Pooling thay v√¨ Flatten**:
  - Gi·∫£m overfitting (√≠t parameters h∆°n)
  - Translation invariance t·ªët h∆°n
  
- **T·ªïng parameters**: ~1.5MB (lightweight, fast inference ~15ms)

**Forward Pass Example**:
```
Input: [1, 3, 224, 224]      # Batch=1, RGB, 224√ó224
  ‚Üì Block1
[1, 32, 112, 112]            # 32 feature maps
  ‚Üì Block2  
[1, 64, 56, 56]
  ‚Üì Block3
[1, 128, 28, 28]
  ‚Üì Block4
[1, 256, 14, 14]
  ‚Üì Global Avg Pool
[1, 256, 1, 1] ‚Üí [1, 256]
  ‚Üì FC
[1, 5]                        # Logits for 5 classes
```

---

### üìó PH·∫¶N 2: Vision Transformer (ViT)

#### 2.1 L√Ω Thuy·∫øt To√°n H·ªçc

**Self-Attention Mechanism** (Core of Transformer):

1. **Input Transformation**:
   ```
   Q = X √ó W_Q    # Query
   K = X √ó W_K    # Key  
   V = X √ó W_V    # Value
   ```
   - X: input embeddings [N, D] (N patches, D dimensions)
   - W_Q, W_K, W_V: learned projection matrices

2. **Attention Scores**:
   ```
   Attention(Q, K, V) = softmax(QK^T / ‚àöd_k) √ó V
   ```
   - QK^T: similarity scores gi·ªØa c√°c patches [N, N]
   - /‚àöd_k: scaling factor (d_k = dimension of K)
   - softmax: normalize th√†nh x√°c su·∫•t
   - √óV: weighted sum theo importance

3. **Multi-Head Attention**:
   ```
   MultiHead(X) = Concat(head_1, ..., head_h) √ó W_O
   head_i = Attention(XW_Q^i, XW_K^i, XW_V^i)
   ```
   - h heads (th∆∞·ªùng h=8 ho·∫∑c 12)
   - M·ªói head h·ªçc different aspects
   - Concat v√† project l·∫°i

4. **Transformer Block**:
   ```
   # Self-Attention
   X' = X + MultiHeadAttention(LayerNorm(X))
   
   # Feed-Forward Network
   X'' = X' + MLP(LayerNorm(X'))
   ```
   - Residual connections (X + ...)
   - LayerNorm tr∆∞·ªõc attention/MLP
   - MLP: 2 FC layers v·ªõi GELU activation

#### 2.2 √Åp D·ª•ng Trong D·ª± √Ån - ViT Small

**Architecture** (timm's `vit_small_patch16_224`):

```
Input Image: 224√ó224√ó3

1. PATCH EMBEDDING
   ‚îú‚îÄ Chia ·∫£nh: 14√ó14 patches (m·ªói patch 16√ó16 pixels)
   ‚îú‚îÄ Flatten: 14√ó14√ó(16√ó16√ó3) ‚Üí 196√ó768
   ‚îî‚îÄ Linear projection: [196, 768]

2. POSITIONAL ENCODING
   ‚îú‚îÄ Learnable position embeddings [196, 768]
   ‚îî‚îÄ X = Patch_Emb + Pos_Emb

3. [CLS] TOKEN
   ‚îú‚îÄ Prepend special token for classification
   ‚îî‚îÄ X = [CLS; X] ‚Üí [197, 768]

4. TRANSFORMER ENCODER (12 layers)
   For each layer:
   ‚îú‚îÄ Multi-Head Attention (8 heads)
   ‚îÇ   ‚îú‚îÄ Q, K, V projections
   ‚îÇ   ‚îú‚îÄ Attention: softmax(QK^T/‚àö96)V
   ‚îÇ   ‚îî‚îÄ Output projection
   ‚îú‚îÄ Residual + LayerNorm
   ‚îú‚îÄ MLP (768 ‚Üí 3072 ‚Üí 768)
   ‚îî‚îÄ Residual + LayerNorm

5. CLASSIFICATION HEAD
   ‚îú‚îÄ Extract [CLS] token: [768]
   ‚îú‚îÄ LayerNorm
   ‚îî‚îÄ Linear: [768] ‚Üí [5 classes]
```

**Key Parameters**:
- Embedding dim: 768
- Num heads: 8 (each head: 768/8 = 96 dims)
- MLP ratio: 4√ó (768 ‚Üí 3072 ‚Üí 768)
- Depth: 12 transformer blocks
- Total params: ~22M ‚Üí ~87MB checkpoint

**T·∫°i sao ViT hi·ªáu qu·∫£?**:

1. **Global receptive field ngay t·ª´ layer 1**:
   - CNN: receptive field tƒÉng d·∫ßn qua layers
   - ViT: m·ªçi patch attend to all patches ngay t·ª´ ƒë·∫ßu
   - ‚Üí H·ªçc long-range dependencies t·ªët h∆°n

2. **Self-attention h·ªçc adaptive**:
   - CNN: fixed kernel weights
   - ViT: attention weights change theo input
   - ‚Üí Flexible h∆°n cho complex patterns

3. **Pretrained on large dataset**:
   - ImageNet-21k (14M images)
   - Learn powerful visual representations
   - Transfer t·ªët sang rice disease classification

**Trade-offs**:
- ‚úÖ Accuracy cao h∆°n CNN (89.2% vs 87.3%)
- ‚úÖ Better v·ªõi complex diseases
- ‚ùå Slow h∆°n (50-100ms vs 15ms)
- ‚ùå Model size l·ªõn (87MB vs 1.5MB)

---

### üìô PH·∫¶N 3: Optimization Algorithms

#### 3.1 Gradient Descent Variants

**1. Vanilla SGD** (Stochastic Gradient Descent):
```python
Œ∏_t = Œ∏_{t-1} - Œ∑ √ó ‚àáL(Œ∏_{t-1})
```
- Œ∑: learning rate
- ‚àáL: gradient c·ªßa loss
- **V·∫•n ƒë·ªÅ**: Oscillation, slow convergence

**2. SGD with Momentum**:
```python
v_t = Œ≤ √ó v_{t-1} + ‚àáL(Œ∏)
Œ∏_t = Œ∏_{t-1} - Œ∑ √ó v_t
```
- Œ≤: momentum coefficient (th∆∞·ªùng 0.9)
- v: velocity (exponential moving average of gradients)
- **L·ª£i √≠ch**: Smooth trajectory, faster convergence

**3. Adam** (Adaptive Moment Estimation):
```python
# First moment (mean)
m_t = Œ≤1 √ó m_{t-1} + (1-Œ≤1) √ó ‚àáL

# Second moment (uncentered variance)
v_t = Œ≤2 √ó v_{t-1} + (1-Œ≤2) √ó (‚àáL)¬≤

# Bias correction
mÃÇ_t = m_t / (1 - Œ≤1^t)
vÃÇ_t = v_t / (1 - Œ≤2^t)

# Update
Œ∏_t = Œ∏_{t-1} - Œ∑ √ó mÃÇ_t / (‚àövÃÇ_t + Œµ)
```
- Œ≤1 = 0.9, Œ≤2 = 0.999, Œµ = 1e-8
- Adaptive learning rate cho m·ªói parameter
- **Best for**: Deep neural networks

**4. AdamW** (Adam with Weight Decay):
```python
# Adam update
Œ∏_t = Œ∏_{t-1} - Œ∑ √ó mÃÇ_t / (‚àövÃÇ_t + Œµ)

# THEN apply weight decay SEPARATELY
Œ∏_t = Œ∏_t - Œ∑ √ó Œª √ó Œ∏_t
```
- Œª: weight decay coefficient
- **T·∫°i sao t√°ch ri√™ng?**: L2 regularization trong Adam b·ªã coupled v·ªõi adaptive learning rate
- **Better for**: Transformers, vision models

#### 3.2 √Åp D·ª•ng Trong D·ª± √Ån

**Training Configuration** (trong `configs/`):

```yaml
# CNN config (cls_cnn_small.yaml)
optimizer:
  type: AdamW
  lr: 1e-4              # Higher lr for CNN
  weight_decay: 1e-4    # L2 regularization
  betas: [0.9, 0.999]

# ViT config (cls_vit_s.yaml)  
optimizer:
  type: AdamW
  lr: 5e-5              # Lower lr for ViT (pretrained)
  weight_decay: 0.05    # Stronger regularization
  betas: [0.9, 0.999]
```

**T·∫°i sao kh√°c nhau?**:

1. **Learning Rate**:
   - CNN: 1e-4 (train from scratch ‚Üí c·∫ßn lr cao h∆°n)
   - ViT: 5e-5 (pretrained ‚Üí fine-tune nh·∫π nh√†ng)

2. **Weight Decay**:
   - CNN: 1e-4 (model nh·ªè, risk overfitting th·∫•p)
   - ViT: 0.05 (model l·ªõn, c·∫ßn regularize m·∫°nh)

**Learning Rate Scheduler**:
```python
CosineAnnealingLR(optimizer, T_max=epochs, eta_min=1e-6)

# LR decay theo cosine:
lr_t = eta_min + (lr_0 - eta_min) √ó (1 + cos(œÄt/T)) / 2
```

Example v·ªõi lr_0=1e-4, T=50 epochs:
```
Epoch  0: lr = 1.00e-4
Epoch 12: lr = 7.07e-5  (gi·∫£m d·∫ßn)
Epoch 25: lr = 5.00e-5  (mid-point)
Epoch 37: lr = 2.93e-5
Epoch 50: lr = 1.00e-6  (eta_min)
```

**L·ª£i √≠ch**:
- Smooth convergence (kh√¥ng c√≥ sudden drops nh∆∞ StepLR)
- Exploration ·ªü ƒë·∫ßu (high lr) ‚Üí exploitation ·ªü cu·ªëi (low lr)

---

### üìï PH·∫¶N 4: Backpropagation Algorithm

#### 4.1 L√Ω Thuy·∫øt

**Chain Rule**:
```
‚àÇL/‚àÇw = ‚àÇL/‚àÇy √ó ‚àÇy/‚àÇz √ó ‚àÇz/‚àÇw
```

**Example v·ªõi 1 FC layer**:
```
Forward:
z = W √ó x + b
y = œÉ(z)         # œÉ = activation (ReLU, softmax, etc.)
L = loss(y, y_true)

Backward:
‚àÇL/‚àÇW = ‚àÇL/‚àÇy √ó ‚àÇy/‚àÇz √ó ‚àÇz/‚àÇW
      = ‚àÇL/‚àÇy √ó œÉ'(z) √ó x^T

‚àÇL/‚àÇb = ‚àÇL/‚àÇy √ó œÉ'(z)

‚àÇL/‚àÇx = W^T √ó (‚àÇL/‚àÇy √ó œÉ'(z))  # Pass to previous layer
```

#### 4.2 Backprop Through CNN

**Convolution Layer**:
```
Forward: Y = Conv(X, W) + b

Backward:
‚àÇL/‚àÇW = Conv(X, ‚àÇL/‚àÇY)           # Gradient w.r.t weights
‚àÇL/‚àÇX = ConvTranspose(‚àÇL/‚àÇY, W)  # Gradient w.r.t input
```

**Max Pooling**:
```
Forward: Y = MaxPool(X)
Backward: ‚àÇL/‚àÇX[i] = {
    ‚àÇL/‚àÇY[j]  if X[i] was the max
    0         otherwise
}
```

#### 4.3 Backprop Through Transformer

**Self-Attention**:
```
Forward:
S = QK^T / ‚àöd
A = softmax(S)
Y = A √ó V

Backward (simplified):
‚àÇL/‚àÇV = A^T √ó ‚àÇL/‚àÇY
‚àÇL/‚àÇA = ‚àÇL/‚àÇY √ó V^T
‚àÇL/‚àÇS = ‚àÇsoftmax/‚àÇS √ó ‚àÇL/‚àÇA
‚àÇL/‚àÇQ = ‚àÇL/‚àÇS √ó K / ‚àöd
‚àÇL/‚àÇK = ‚àÇL/‚àÇS^T √ó Q / ‚àöd
```

**Residual Connection**:
```
Forward: Y = X + F(X)
Backward: ‚àÇL/‚àÇX = ‚àÇL/‚àÇY √ó (1 + ‚àÇF/‚àÇX)
```
- Gradient flow tr·ª±c ti·∫øp qua "1" ‚Üí tr√°nh vanishing

**LayerNorm**:
```
Forward: y = Œ≥ √ó (x - Œº) / œÉ + Œ≤
Backward: c·∫ßn t√≠nh ‚àÇL/‚àÇx qua normalization
```
- Ph·ª©c t·∫°p nh∆∞ng stable gradients

#### 4.4 √Åp D·ª•ng Trong Training Loop

**Trong `src/core/engine.py`**:
```python
def train_one_epoch(model, dataloader, criterion, optimizer):
    for images, labels in dataloader:
        # 1. FORWARD PASS
        logits = model(images)          # CNN ho·∫∑c ViT
        loss = criterion(logits, labels)  # CrossEntropyLoss
        
        # 2. BACKWARD PASS
        optimizer.zero_grad()           # Reset gradients
        loss.backward()                 # Backpropagation
        
        # 3. GRADIENT CLIPPING (optional)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        
        # 4. OPTIMIZER STEP
        optimizer.step()                # Update weights
```

**CrossEntropyLoss Backward**:
```
Loss = -Œ£ y_true[i] √ó log(softmax(logits)[i])

‚àÇLoss/‚àÇlogits = softmax(logits) - y_true
```
- ƒê∆°n gi·∫£n v√† numerically stable

---

### üìí PH·∫¶N 5: Regularization Techniques

#### 5.1 Weight Decay (L2 Regularization)

**L√Ω thuy·∫øt**:
```
Loss_total = Loss_data + Œª/2 √ó Œ£ w¬≤

‚àÇLoss_total/‚àÇw = ‚àÇLoss_data/‚àÇw + Œª √ó w
```

**Trong AdamW** (d·ª± √°n d√πng):
```python
# Standard update
Œ∏ = Œ∏ - lr √ó gradient

# With weight decay
Œ∏ = Œ∏ √ó (1 - lr √ó Œª) - lr √ó gradient
```

**Trong d·ª± √°n**:
- CNN: Œª = 1e-4 (mild regularization)
- ViT: Œª = 0.05 (strong regularization v√¨ model l·ªõn)

#### 5.2 Dropout

**L√Ω thuy·∫øt**:
```
Training: y = DropOut(x, p=0.5) = {
    0        with probability p
    x/(1-p)  with probability 1-p
}

Inference: y = x  (no dropout)
```

**Kh√¥ng d√πng trong d·ª± √°n n√†y** v√¨:
- CNN: BatchNorm ƒë√£ regularize t·ªët
- ViT: Pretrained + weight decay ƒë·ªß

#### 5.3 Data Augmentation (Implicit Regularization)

**Trong `datasets_cls.py`**:
```python
train_transform = transforms.Compose([
    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),
    transforms.RandomRotation(15),
    transforms.ToTensor(),
    transforms.Normalize(...)
])
```

**Hi·ªáu ·ª©ng**: M·ªói epoch model th·∫•y "·∫£nh m·ªõi" ‚Üí generalize t·ªët h∆°n

---

### üéì PH·∫¶N 6: ·ª®ng D·ª•ng T·ªïng H·ª£p

#### Quy Tr√¨nh Training End-to-End

```python
# INITIALIZATION
model = SmallCNN(num_classes=5)  # ho·∫∑c ViT
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)
scheduler = CosineAnnealingLR(optimizer, T_max=50)

# TRAINING LOOP
for epoch in range(num_epochs):
    # TRAINING PHASE
    model.train()
    for batch in train_loader:
        images, labels = batch
        
        # Forward: s·ª≠ d·ª•ng CNN/ViT algorithms
        logits = model(images)
        loss = criterion(logits, labels)
        
        # Backward: backpropagation algorithm
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
    
    # VALIDATION PHASE
    model.eval()
    with torch.no_grad():  # Kh√¥ng t√≠nh gradient
        for batch in val_loader:
            logits = model(images)
            # Compute metrics...
    
    # Learning rate decay
    scheduler.step()
    
    # Early stopping check
    if val_f1 > best_f1:
        best_f1 = val_f1
        save_checkpoint()
        patience_counter = 0
    else:
        patience_counter += 1
        if patience_counter >= 10:
            break  # Stop training
```

**K·∫øt n·ªëi c√°c thu·∫≠t to√°n**:
1. **CNN/ViT**: Feature extraction
2. **Softmax**: Convert logits ‚Üí probabilities
3. **CrossEntropy**: Loss computation
4. **Backpropagation**: Compute gradients
5. **AdamW**: Update weights
6. **CosineAnnealing**: Adjust learning rate

---

## üî¨ C√¢u H·ªèi B·ªï Sung V·ªÅ Thu·∫≠t To√°n

#### 31. **Gi·∫£i th√≠ch chi ti·∫øt softmax v√† t·∫°i sao d√πng trong classification.**
**Tr·∫£ l·ªùi**:
```python
softmax(z_i) = exp(z_i) / Œ£ exp(z_j)
```

**T√≠nh ch·∫•t**:
- Output: x√°c su·∫•t [0, 1], t·ªïng = 1
- Differentiable: backprop ƒë∆∞·ª£c
- Amplify differences: class c√≥ logit cao ‚Üí probability cao h∆°n nhi·ªÅu

**Trong d·ª± √°n**:
```python
logits = model(image)  # [1, 5]: [2.3, 5.1, 1.2, 0.8, 3.4]
probs = softmax(logits) # [0.04, 0.66, 0.01, 0.01, 0.12]
predicted = argmax(probs)  # Class 1 (index 1)
```

#### 32. **Residual connection gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ g√¨?**
**Tr·∫£ l·ªùi**:
```python
# Without residual: y = F(x)
# Gradient: ‚àÇL/‚àÇx = ‚àÇL/‚àÇy √ó ‚àÇF/‚àÇx

# With residual: y = x + F(x)
# Gradient: ‚àÇL/‚àÇx = ‚àÇL/‚àÇy √ó (1 + ‚àÇF/‚àÇx)
```

**Vanishing Gradient Problem**:
- Deep networks: gradient √ó √ó √ó ‚Üí 0
- Residual: lu√¥n c√≥ "1" trong gradient ‚Üí flow t·ªët

**Trong ViT**: M·ªói transformer block c√≥ 2 residual connections

#### 33. **Batch Normalization vs Layer Normalization?**
**Tr·∫£ l·ªùi**:

**Batch Norm** (CNN d√πng):
```
Œº_batch = mean(X across batch dimension)
BN(x) = (x - Œº_batch) / œÉ_batch
```
- Normalize theo batch ‚Üí require large batch size
- D√πng trong CNN (spatial dimensions t∆∞∆°ng ƒë·ªìng)

**Layer Norm** (ViT d√πng):
```
Œº_layer = mean(X across feature dimension)  
LN(x) = (x - Œº_layer) / œÉ_layer
```
- Normalize theo features ‚Üí independent of batch size
- Better cho Transformers (sequence length vary)

**Ch√∫c b·∫°n hi·ªÉu s√¢u v·ªÅ thu·∫≠t to√°n v√† √°p d·ª•ng t·ªët! üöÄ**
