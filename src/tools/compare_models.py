# src/tools/compare_models.py
"""
Comprehensive model comparison utility for CNN vs ViT.
Evaluates both models on test set and generates detailed comparison report.

Usage:
    python -m src.tools.compare_models
    python -m src.tools.compare_models --split_file data/splits/val_cls.txt
"""
import argparse
import os
import time
import csv
from collections import defaultdict
import torch
from torchvision import transforms
from PIL import Image
import timm
import numpy as np
from sklearn.metrics import (
    precision_recall_fscore_support,
    confusion_matrix,
    accuracy_score,
)
from src.models.cnn_small import SmallCNN


def load_class_map(path):
    with open(path, "r", encoding="utf-8") as f:
        return [l.strip() for l in f if l.strip()]


def build_val_tf(sz):
    return transforms.Compose([
        transforms.Resize((sz, sz)),
        transforms.ToTensor(),
        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),
    ])


def load_model(ckpt, model_name, num_classes, device):
    if model_name == "cnn_small":
        model = SmallCNN(num_classes)
    else:
        model = timm.create_model(model_name, pretrained=False, num_classes=num_classes)
    
    try:
        sd = torch.load(ckpt, map_location=device, weights_only=False)
    except TypeError:
        sd = torch.load(ckpt, map_location=device)
    
    if isinstance(sd, dict) and "model" in sd:
        sd = sd["model"]
    
    model.load_state_dict(sd)
    model.to(device).eval()
    return model


def parse_split_line(line):
    import re
    s = line.strip()
    if not s or s.startswith("#"):
        return "", None
    m = re.search(r"[ \t,](\d+)$", s)
    if not m:
        raise ValueError(f"Bad line: {line}")
    lab = int(m.group(1))
    p = s[: m.start()].strip()
    # Normalize path
    p = p.strip().strip('"').strip("'").replace("\\", "/")
    if p.lower().startswith("public/rice_cls/"):
        p = "data/" + p[len("public/"):]
    elif p.lower().startswith("rice_cls/"):
        p = "data/" + p
    return p, lab


def parse_split(fp):
    paths, labels = [], []
    with open(fp, "r", encoding="utf-8") as f:
        for raw in f:
            p, lab = parse_split_line(raw)
            if p == "" and lab is None:
                continue
            paths.append(p)
            labels.append(int(lab))
    return paths, labels


def evaluate_model(model, paths, y_true, transform, device, class_names):
    """Evaluate a single model and return predictions + timings."""
    y_pred = []
    times = []
    
    for p in paths:
        img = Image.open(p).convert("RGB")
        x = transform(img).unsqueeze(0).to(device)
        
        start = time.time()
        with torch.no_grad():
            logits = model(x)
            pred = logits.argmax(1).item()
        elapsed = (time.time() - start) * 1000  # ms
        
        y_pred.append(pred)
        times.append(elapsed)
    
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)
    
    # Calculate metrics
    prec, rec, f1, sup = precision_recall_fscore_support(
        y_true, y_pred, labels=np.arange(len(class_names)), zero_division=0
    )
    acc = accuracy_score(y_true, y_pred)
    
    prec_macro, rec_macro, f1_macro, _ = precision_recall_fscore_support(
        y_true, y_pred, average="macro", zero_division=0
    )
    
    cm = confusion_matrix(y_true, y_pred, labels=np.arange(len(class_names)))
    
    return {
        "predictions": y_pred,
        "accuracy": acc,
        "precision_macro": prec_macro,
        "recall_macro": rec_macro,
        "f1_macro": f1_macro,
        "per_class_metrics": {
            "precision": prec,
            "recall": rec,
            "f1": f1,
            "support": sup
        },
        "confusion_matrix": cm,
        "inference_times": times,
        "avg_time": np.mean(times),
        "std_time": np.std(times)
    }


def print_comparison_report(cnn_results, vit_results, class_names, paths, y_true):
    """Print comprehensive comparison report."""
    print("\n" + "="*80)
    print("üìä B√ÅO C√ÅO SO S√ÅNH MODEL: CNN vs ViT")
    print("="*80)
    
    # Overall metrics
    print("\n1Ô∏è‚É£  T·ªîNG QUAN HI·ªÜU SU·∫§T\n")
    print(f"{'Metric':<25} {'CNN':<15} {'ViT':<15} {'Winner':<10}")
    print("-"*70)
    
    metrics = [
        ("Accuracy", "accuracy"),
        ("F1 Score (Macro)", "f1_macro"),
        ("Precision (Macro)", "precision_macro"),
        ("Recall (Macro)", "recall_macro"),
    ]
    
    for metric_name, key in metrics:
        cnn_val = cnn_results[key]
        vit_val = vit_results[key]
        winner = "ViT ‚úì" if vit_val > cnn_val else "CNN ‚úì" if cnn_val > vit_val else "Tie"
        print(f"{metric_name:<25} {cnn_val:<15.4f} {vit_val:<15.4f} {winner:<10}")
    
    # Speed comparison
    print("\n2Ô∏è‚É£  HI·ªÜU SU·∫§T T·ªêC ƒê·ªò\n")
    print(f"{'Metric':<25} {'CNN':<15} {'ViT':<15}")
    print("-"*55)
    print(f"{'Avg time/image (ms)':<25} {cnn_results['avg_time']:<15.2f} {vit_results['avg_time']:<15.2f}")
    print(f"{'Std time (ms)':<25} {cnn_results['std_time']:<15.2f} {vit_results['std_time']:<15.2f}")
    speedup = vit_results['avg_time'] / cnn_results['avg_time']
    print(f"\n‚ö° CNN nhanh h∆°n ViT: {speedup:.2f}x")
    
    # Per-class comparison
    print("\n3Ô∏è‚É£  SO S√ÅNH THEO T·ª™NG L·ªöP\n")
    print(f"{'Class':<18} {'CNN F1':<10} {'ViT F1':<10} {'Diff':<10} {'Winner':<10}")
    print("-"*60)
    
    for i, cls_name in enumerate(class_names):
        cnn_f1 = cnn_results['per_class_metrics']['f1'][i]
        vit_f1 = vit_results['per_class_metrics']['f1'][i]
        diff = vit_f1 - cnn_f1
        winner = "ViT ‚úì" if diff > 0.01 else "CNN ‚úì" if diff < -0.01 else "~Tie"
        print(f"{cls_name:<18} {cnn_f1:<10.4f} {vit_f1:<10.4f} {diff:+<10.4f} {winner:<10}")
    
    # Agreement analysis
    print("\n4Ô∏è‚É£  PH√ÇN T√çCH ƒê·ªíNG THU·∫¨N\n")
    agreements = (cnn_results['predictions'] == vit_results['predictions'])
    agree_rate = np.mean(agreements) * 100
    
    print(f"T·ª∑ l·ªá ƒë·ªìng thu·∫≠n: {agree_rate:.2f}% ({np.sum(agreements)}/{len(agreements)})")
    
    # Analyze disagreements
    disagree_indices = np.where(~agreements)[0]
    if len(disagree_indices) > 0:
        print(f"\nüîç Ph√¢n t√≠ch {len(disagree_indices)} tr∆∞·ªùng h·ª£p B·∫§T ƒê·ªíNG:\n")
        
        # Group by pattern
        patterns = defaultdict(list)
        for idx in disagree_indices[:10]:  # Show first 10
            true_label = y_true[idx]
            cnn_pred = cnn_results['predictions'][idx]
            vit_pred = vit_results['predictions'][idx]
            
            cnn_correct = (cnn_pred == true_label)
            vit_correct = (vit_pred == true_label)
            
            if cnn_correct and not vit_correct:
                pattern = "CNN ƒë√∫ng, ViT sai"
            elif vit_correct and not cnn_correct:
                pattern = "ViT ƒë√∫ng, CNN sai"
            else:
                pattern = "C·∫£ 2 ƒë·ªÅu sai"
            
            patterns[pattern].append({
                'path': paths[idx],
                'true': class_names[true_label],
                'cnn': class_names[cnn_pred],
                'vit': class_names[vit_pred]
            })
        
        for pattern, cases in patterns.items():
            print(f"\n  {pattern}: {len(cases)} case(s)")
            for case in cases[:3]:  # Show first 3 of each pattern
                print(f"    File: {case['path']}")
                print(f"    True: {case['true']}, CNN: {case['cnn']}, ViT: {case['vit']}")
    
    # Recommendations
    print("\n" + "="*80)
    print("üí° KHUY·∫æN NGH·ªä")
    print("="*80)
    
    f1_diff = vit_results['f1_macro'] - cnn_results['f1_macro']
    
    if f1_diff > 0.05 and speedup < 5:
        print("‚úÖ N√™n d√πng ViT: Ch√≠nh x√°c h∆°n ƒë√°ng k·ªÉ, t·ªëc ƒë·ªô ch·∫•p nh·∫≠n ƒë∆∞·ª£c")
    elif f1_diff > 0.05 and speedup >= 5:
        print("‚öñÔ∏è  Trade-off: ViT ch√≠nh x√°c h∆°n nh∆∞ng CNN nhanh h∆°n nhi·ªÅu")
        print(f"   ‚Üí D√πng ViT cho accuracy, CNN cho real-time")
    elif abs(f1_diff) <= 0.05:
        print("‚úÖ N√™n d√πng CNN: Hi·ªáu su·∫•t t∆∞∆°ng ƒë∆∞∆°ng nh∆∞ng nhanh h∆°n r·∫•t nhi·ªÅu")
    else:
        print("‚úÖ N√™n d√πng CNN: V·ª´a ch√≠nh x√°c h∆°n v·ª´a nhanh h∆°n")
    
    print("\nüìå L∆∞u √Ω cho b√°o c√°o cu·ªëi k·ª≥:")
    print("  ‚Ä¢ CNN nh·∫π h∆°n (~1.5MB vs ~87MB), ph√π h·ª£p tri·ªÉn khai th·ª±c t·∫ø")
    print("  ‚Ä¢ ViT th·ªÉ hi·ªán kh·∫£ nƒÉng h·ªçc global context t·ªët h∆°n")
    print(f"  ‚Ä¢ C·∫£ 2 model ƒë·∫°t F1 > 0.80 (y√™u c·∫ßu m√¥n h·ªçc)")
    print("  ‚Ä¢ C√≥ th·ªÉ ensemble 2 model ƒë·ªÉ tƒÉng ƒë·ªô tin c·∫≠y")


def main():
    parser = argparse.ArgumentParser(description="So s√°nh hi·ªáu su·∫•t CNN vs ViT")
    parser.add_argument("--split_file", default="data/splits/test_cls.txt",
                       help="File ch·ª©a test set")
    parser.add_argument("--labels_file", default="data/splits/labels.txt",
                       help="File ch·ª©a t√™n c√°c l·ªõp")
    parser.add_argument("--cnn_checkpoint", default="runs/cls_cnn_small/weights/cnn_small_best.pt")
    parser.add_argument("--vit_checkpoint", default="runs/cls_vit_s_224/weights/vit_small_patch16_224_best.pt")
    parser.add_argument("--out_csv", default="model_comparison.csv",
                       help="File CSV l∆∞u chi ti·∫øt so s√°nh")
    args = parser.parse_args()
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"üîß Device: {device}")
    
    # Load class names
    class_names = load_class_map(args.labels_file)
    num_classes = len(class_names)
    print(f"üìö S·ªë l·ªõp: {num_classes} - {class_names}")
    
    # Load test data
    paths, y_true = parse_split(args.split_file)
    print(f"üìÇ S·ªë m·∫´u test: {len(paths)}\n")
    
    # Load models
    print("üì¶ ƒêang load CNN model...")
    cnn_model = load_model(args.cnn_checkpoint, "cnn_small", num_classes, device)
    cnn_transform = build_val_tf(224)
    
    print("üì¶ ƒêang load ViT model...")
    vit_model = load_model(args.vit_checkpoint, "vit_small_patch16_224", num_classes, device)
    vit_transform = build_val_tf(224)
    
    # Evaluate both models
    print("\n‚è≥ ƒêang ƒë√°nh gi√° CNN model...")
    cnn_results = evaluate_model(cnn_model, paths, y_true, cnn_transform, device, class_names)
    
    print("‚è≥ ƒêang ƒë√°nh gi√° ViT model...")
    vit_results = evaluate_model(vit_model, paths, y_true, vit_transform, device, class_names)
    
    # Print comparison report
    print_comparison_report(cnn_results, vit_results, class_names, paths, y_true)
    
    # Save detailed comparison to CSV
    print(f"\nüíæ ƒêang l∆∞u chi ti·∫øt v√†o {args.out_csv}...")
    with open(args.out_csv, "w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["path", "true_label", "cnn_pred", "vit_pred", 
                        "cnn_correct", "vit_correct", "agree"])
        
        for i, path in enumerate(paths):
            true_label = class_names[y_true[i]]
            cnn_pred = class_names[cnn_results['predictions'][i]]
            vit_pred = class_names[vit_results['predictions'][i]]
            cnn_correct = (cnn_results['predictions'][i] == y_true[i])
            vit_correct = (vit_results['predictions'][i] == y_true[i])
            agree = (cnn_results['predictions'][i] == vit_results['predictions'][i])
            
            writer.writerow([path, true_label, cnn_pred, vit_pred,
                           cnn_correct, vit_correct, agree])
    
    print(f"‚úÖ Ho√†n th√†nh! ƒê√£ l∆∞u {args.out_csv}")


if __name__ == "__main__":
    main()
